#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# @author: wisdom-pan <wisdompan1@outlook.com>
# @date: 2024/08/22
#
import os
os.environ['PT_SDPA_ENABLE_HEAD_DIM_PADDING'] = '1'

import tqdm
import hashlib
import fitz
import gradio as gr
import spaces
from transformers import AutoModel
from transformers import AutoTokenizer
from PIL import Image
import torch
import numpy as np
import json
import cv2
os.environ['HF_ENDPOINT'] = 'https://hf-api.gitee.com'
#æ‰§è¡Œä¸Šé¢è„šæœ¬æ—¶ï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ï¼š
# os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
os.environ['HF_HOME'] = '~/.cache/gitee-ai'


cache_dir = '/data/kb_cache'
os.makedirs(cache_dir, exist_ok=True)


def get_image_md5(img: Image.Image):
    img_byte_array = img.tobytes()
    hash_md5 = hashlib.md5()
    hash_md5.update(img_byte_array)
    hex_digest = hash_md5.hexdigest()
    return hex_digest


def calculate_md5_from_binary(binary_data):
    hash_md5 = hashlib.md5()
    hash_md5.update(binary_data)
    return hash_md5.hexdigest()


@spaces.GPU(duration=100)
def add_pdf_gradio(pdf_file_binary, progress=gr.Progress()):
    global model, tokenizer
    model.eval()

    knowledge_base_name = calculate_md5_from_binary(pdf_file_binary)

    this_cache_dir = os.path.join(cache_dir, knowledge_base_name)
    os.makedirs(this_cache_dir, exist_ok=True)

    with open(os.path.join(this_cache_dir, f"src.pdf"), 'wb') as file:
        file.write(pdf_file_binary)

    dpi = 200
    doc = fitz.open("pdf", pdf_file_binary)

    reps_list = []
    images = []
    image_md5s = []

    for page in progress.tqdm(doc):
        # with self.lock: # because we hope one 16G gpu only process one image at the same time
        pix = page.get_pixmap(dpi=dpi)
        image = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        image_md5 = get_image_md5(image)
        image_md5s.append(image_md5)
        with torch.no_grad():
            reps = model(text=[''], image=[image], tokenizer=tokenizer).reps
        reps_list.append(reps.squeeze(0).cpu().numpy())
        images.append(image)

    for idx in range(len(images)):
        image = images[idx]
        image_md5 = image_md5s[idx]
        cache_image_path = os.path.join(this_cache_dir, f"{image_md5}.png")
        image.save(cache_image_path)

    np.save(os.path.join(this_cache_dir, f"reps.npy"), reps_list)

    with open(os.path.join(this_cache_dir, f"md5s.txt"), 'w') as f:
        for item in image_md5s:
            f.write(item + '\n')

    return knowledge_base_name

@spaces.GPU(duration=100)
def add_video_gradio(video_file_binary, progress=gr.Progress()):
    global model, tokenizer
    model.eval()

    knowledge_base_name = calculate_md5_from_binary(video_file_binary)

    this_cache_dir = os.path.join(cache_dir, knowledge_base_name)
    os.makedirs(this_cache_dir, exist_ok=True)

    video_path = os.path.join(this_cache_dir, f"src.mp4")
    with open(video_path, 'wb') as file:
        file.write(video_file_binary)

    cap = cv2.VideoCapture(video_path)
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = int(cap.get(cv2.CAP_PROP_FPS))

    reps_list = []
    images = []
    image_md5s = []

    for i in progress.tqdm(range(frame_count)):
        ret, frame = cap.read()
        if not ret:
            break
        image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        image_md5 = get_image_md5(image)
        image_md5s.append(image_md5)
        with torch.no_grad():
            reps = model(text=[''], image=[image], tokenizer=tokenizer).reps
        reps_list.append(reps.squeeze(0).cpu().numpy())
        images.append(image)

    cap.release()

    for idx in range(len(images)):
        image = images[idx]
        image_md5 = image_md5s[idx]
        cache_image_path = os.path.join(this_cache_dir, f"{image_md5}.png")
        image.save(cache_image_path)

    np.save(os.path.join(this_cache_dir, f"reps.npy"), reps_list)

    with open(os.path.join(this_cache_dir, f"md5s.txt"), 'w') as f:
        for item in image_md5s:
            f.write(item + '\n')

    return knowledge_base_name


@spaces.GPU
def retrieve_gradio(knowledge_base: str, query: str, topk: int):
    global model, tokenizer

    model.eval()

    target_cache_dir = os.path.join(cache_dir, knowledge_base)

    if not os.path.exists(target_cache_dir):
        return None

    md5s = []
    with open(os.path.join(target_cache_dir, f"md5s.txt"), 'r') as f:
        for line in f:
            md5s.append(line.rstrip('\n'))

    doc_reps = np.load(os.path.join(target_cache_dir, f"reps.npy"))

    query_with_instruction = "Represent this query for retrieving relavant document: " + query
    with torch.no_grad():
        query_rep = model(text=[query_with_instruction], image=[None], tokenizer=tokenizer).reps.squeeze(0).cpu()

    query_md5 = hashlib.md5(query.encode()).hexdigest()

    doc_reps_cat = torch.stack([torch.Tensor(i) for i in doc_reps], dim=0)

    similarities = torch.matmul(query_rep, doc_reps_cat.T)

    topk_values, topk_doc_ids = torch.topk(similarities, k=topk)

    topk_values_np = topk_values.cpu().numpy()

    topk_doc_ids_np = topk_doc_ids.cpu().numpy()

    similarities_np = similarities.cpu().numpy()

    images_topk = [Image.open(os.path.join(target_cache_dir, f"{md5s[idx]}.png")) for idx in topk_doc_ids_np]

    with open(os.path.join(target_cache_dir, f"q-{query_md5}.json"), 'w') as f:
        f.write(json.dumps(
            {
                "knowledge_base": knowledge_base,
                "query": query,
                "retrived_docs": [os.path.join(target_cache_dir, f"{md5s[idx]}.png") for idx in topk_doc_ids_np]
            }, indent=4, ensure_ascii=False
        ))

    return images_topk


def upvote(knowledge_base, query):
    """
    æ›´æ–°çŸ¥è¯†åº“ä¸­æŒ‡å®šæŸ¥è¯¢çš„ç”¨æˆ·åå¥½ä¸ºâ€œupvoteâ€ï¼Œå¹¶å°†ç»“æœä¿å­˜åˆ°æ–°çš„JSONæ–‡ä»¶ä¸­ã€‚

    å‚æ•°:
    knowledge_base (str): çŸ¥è¯†åº“çš„åç§°æˆ–è·¯å¾„ã€‚
    query (str): ç”¨æˆ·æŸ¥è¯¢çš„å­—ç¬¦ä¸²ã€‚
    """
    global model, tokenizer

    target_cache_dir = os.path.join(cache_dir, knowledge_base)

    query_md5 = hashlib.md5(query.encode()).hexdigest()

    with open(os.path.join(target_cache_dir, f"q-{query_md5}.json"), 'r') as f:
        data = json.loads(f.read())

    data["user_preference"] = "upvote"

    with open(os.path.join(target_cache_dir, f"q-{query_md5}-withpref.json"), 'w') as f:
        f.write(json.dumps(data, indent=4, ensure_ascii=False))

    print("up", os.path.join(target_cache_dir, f"q-{query_md5}-withpref.json"))

    gr.Info('Received, babe! Thank you!')

    return


def downvote(knowledge_base, query):
    global model, tokenizer

    target_cache_dir = os.path.join(cache_dir, knowledge_base)

    query_md5 = hashlib.md5(query.encode()).hexdigest()

    with open(os.path.join(target_cache_dir, f"q-{query_md5}.json"), 'r') as f:
        data = json.loads(f.read())

    data["user_preference"] = "downvote"

    with open(os.path.join(target_cache_dir, f"q-{query_md5}-withpref.json"), 'w') as f:
        f.write(json.dumps(data, indent=4, ensure_ascii=False))

    print("down", os.path.join(target_cache_dir, f"q-{query_md5}-withpref.json"))

    gr.Info('Received, babe! Thank you!')

    return


device = 'cuda'

print("emb model load begin...")
model_path = 'RhapsodyAI/minicpm-visual-embedding-v0'  # replace with your local model path
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = AutoModel.from_pretrained(model_path, trust_remote_code=True)
model.eval()
model.to(device)
print("emb model load success!")

print("gen model load begin...")
gen_model_path = 'openbmb/MiniCPM-V-2_6'
gen_tokenizer = AutoTokenizer.from_pretrained(gen_model_path, trust_remote_code=True)
# gen_model = AutoModel.from_pretrained(gen_model_path, trust_remote_code=True, attn_implementation='sdpa',
#                                       torch_dtype=torch.bfloat16)
gen_model = AutoModel.from_pretrained(gen_model_path, trust_remote_code=True,torch_dtype=torch.bfloat16)
gen_model.eval()
gen_model.to(device)
print("gen model load success!")


@spaces.GPU(duration=50)
def answer_question(images, question):
    global gen_model, gen_tokenizer
    # here each element of images is a tuple of (image_path, None).
    images_ = [Image.open(image[0]).convert('RGB') for image in images]
    msgs = [{'role': 'user', 'content': [question, *images_]}]
    answer = gen_model.chat(
        image=None,
        msgs=msgs,
        tokenizer=gen_tokenizer
    )
    print(answer)
    return answer


with gr.Blocks() as app:
    gr.Markdown("#ä½ çš„ä¸ªäººå›¾ä¹¦é¦†ç®¡å®¶FancyLibrary")

    gr.Markdown("""
- **æå‡ºé—®é¢˜**ï¼Œå®ƒä¼šæ£€ç´¢æœ€ç›¸å…³çš„é¡µé¢ï¼Œå°†æ ¹æ®è¿”å›top kçš„é¡µé¢å›ç­”æ‚¨çš„é—®é¢˜ã€‚

    -å¯ä»¥å¸®åŠ©æ‚¨é˜…è¯»è¾ƒé•¿çš„å›¾åƒã€æ–‡æœ¬ç±»çš„PDFä»¥åŠè§†é¢‘ä¸­çš„ä¸é—®é¢˜ç›¸å…³çš„å…³é”®å¸§å¹¶æ‰¾åˆ°å›ç­”æ‚¨é—®é¢˜çš„é¡µé¢ã€‚

    - å¯ä»¥å¸®åŠ©æ‚¨å»ºç«‹ä¸ªäººå›¾ä¹¦é¦†å¹¶ä»å¤šæ¨¡æ€æ•°æ®ä¸­æœ€ç›¸å…³çš„é¡µé¢ç”¨äºRAGé—®ç­”ç³»ç»Ÿã€‚

    - ç”¨å®Œæ•´çš„è§†è§‰é˜…è¯»ã€å­˜å‚¨ã€æ£€ç´¢å’Œå›ç­”.
""")

    # gr.Markdown(
    #     "- Currently online demo support PDF document with less than 50 pages due to GPU time limit. Deploy on your own machine for longer PDFs and books.")

    with gr.Row():
        file_input = gr.File(type="binary", label="ä¸Šä¼ ä½ çš„æ–‡ä»¶")
        file_result = gr.Text(label="çŸ¥è¯†åº“ID ")
        process_button = gr.Button("å¤„ç† PDFï¼ˆPDF ä¸Šä¼ æˆåŠŸåæ‰å¯ç‚¹å‡»ï¼‰")

    process_button.click(add_pdf_gradio, inputs=[file_input], outputs=file_result)

    with gr.Row():
        video_input = gr.File(type="binary", label="ä¸Šä¼ ä½ çš„è§†é¢‘")
        video_result = gr.Text(label="çŸ¥è¯†åº“ID ")
        process_video_button = gr.Button("å¤„ç†è§†é¢‘ï¼ˆè§†é¢‘ä¸Šä¼ æˆåŠŸåæ‰å¯ç‚¹å‡»ï¼‰")

    process_video_button.click(add_video_gradio, inputs=[video_input], outputs=video_result)

    with gr.Row():
        kb_id_input = gr.Text(label="æ‚¨çš„çŸ¥è¯†åº“ IDï¼ˆåœ¨æ­¤å¤„ç²˜è´´æ‚¨çš„çŸ¥è¯†åº“ IDï¼Œå¯é‡å¤ä½¿ç”¨ï¼šï¼‰")
        query_input = gr.Text(label="è¾“å…¥ä½ çš„é—®é¢˜")
        topk_input = inputs = gr.Number(value=5, minimum=1, maximum=10, step=1, label="è¦æ£€ç´¢çš„é¡µæ•°ï¼Œ1-10é¡µ")
        retrieve_button = gr.Button("æ­¥éª¤2ï¼šæ£€ç´¢é¡µé¢")

    with gr.Row():
        images_output = gr.Gallery(label="æ£€ç´¢é¡µé¢")
        retrieve_button.click(retrieve_gradio, inputs=[kb_id_input, query_input, topk_input], outputs=images_output)

    with gr.Row():
        button = gr.Button("æ­¥éª¤ 3ï¼šä½¿ç”¨æ£€ç´¢åˆ°çš„é¡µé¢å›ç­”é—®é¢˜")

        gen_model_response = gr.Textbox(label="æ£€ç´¢å“åº”")

        button.click(fn=answer_question, inputs=[images_output, query_input], outputs=gen_model_response)

    with gr.Row():
        downvote_button = gr.Button("ğŸ‘èµä¸€ä¸‹")
        upvote_button = gr.Button("ğŸ‘è¸©ä¸€ä¸‹")

    upvote_button.click(upvote, inputs=[kb_id_input, query_input], outputs=None)
    downvote_button.click(downvote, inputs=[kb_id_input, query_input], outputs=None)



app.launch()

